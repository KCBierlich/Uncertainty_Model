---
title: "Posterior diagnostics for drone data"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
      number_sections: true
params:
  samples: output/mcmc/fit_mns_laser.rds
  nburn: 1e2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = FALSE, message = FALSE, 
                      warning = FALSE)
```

Posterior Diagnostic Report updated to help with a speedier interpretation of the larger amount of data and measurements that are input into the model.  
by KC Bierlich  
10/7/20  
  
# Configuration

```{r load_model, include = FALSE}
library(nimble)

## Load R files and subplans
lapply(list.files("../R", full.names = TRUE, recursive = TRUE), source)

# load inputs to target model
nim_pkg = readRDS(file.path('..', params$samples))$config

# build nimble model, so we can extract priors
droneLengths = nimbleModel(code = model, constants = nim_pkg$consts,
                           data = nim_pkg$data, inits = nim_pkg$inits,
                           name = 'droneLengths')
```

```{r load_samples, include = FALSE}
samples = readRDS(file.path('..', params$samples))$samples   # name of samples - measurements, altitude
samples.names = colnames(samples)                          # their names, i.e., L[1], a[1], bias[1], sigma[1]
burn = 1:params$nburn
```

This report was generated by the target 
<span style="color:blue">`r id_chr()`</span>, which displays
posterior diagnostics for the samples in 
<span style="color:blue">`r basename(params$samples)`</span>.  The file 
contains <span style="color:blue">`r format(nrow(samples), big.mark = ',')`</span> posterior samples, and estimation used <span style="color:blue">`r paste(grep('[A-z]+', c(ifelse(nim_pkg$consts$useBarometer, 'barometer', ''), ifelse(nim_pkg$consts$useLaser, 'laser', '')), value = TRUE), collapse = ' and ')`</span> altimeter data.


# Posterior learning 

## Tabs {.tabset}

### Summary table

#### Posterior means and HPD intervals

```{r summary_table}
library(coda)

tgt = c('sigma[3]')
param_names = c('sigma[Pixels]')

if(nim_pkg$consts$useBarometer) {
  tgt = c('sigma[1]', tgt)
  param_names = c('sigma[Barometer]', param_names)
}

if(nim_pkg$consts$useLaser) {
  tgt = c('sigma[2]', tgt)
  param_names = c('sigma[Laser]', param_names)
}


m = mcmc(samples[-burn, tgt])
colnames(m) = param_names
#mcmc(samples[-burn, c("L[2]", "L[3]")])
s = summary(m)

round(s$statistics, 2)


round(HPDinterval(m), 2)
```

#### Effective sample sizes

```{r ess}
effectiveSize(m)
```


#### Posterior correlations

```{r correlations}
# posterior correlations
m.cor = cor(m)

# full matrix
round(m.cor, 2)
```

```{r prior_post_plot_fn, include = FALSE}
library(ggplot2)
library(ggthemes)
library(coda)

distn_info = function(pattern, model = NULL, samples, burn, 
                      output = c('prior', 'prior_post', 'trace')) {
  # Parameters:
  #  pattern - string to select nodes
  #  model - nimble model, needed to automatically extract priors
  #  samples - posterior samples
  #  burn - number of samples to discard
  
  # extract model nodes that match the pattern
  tgt = grep(pattern = pattern, x = colnames(samples), value = TRUE)
  
  if(length(burn) == 1) {
    burn = 1:burn
  }
  
  # process matching nodes
  res = lapply(tgt, function(node) {
    
    # initialize output for node
    res = list()
    
    if(TRUE) {
      if(!is.null(model)) {
        
        if(any(c('prior','prior_post') %in% output)) {
          # extract name of distribution for node
          distn = model$getDistribution(node)
          
          # get names of parameters for distribution
          params.names = names(model$getDimension(node, 
                                                  includeParams = TRUE))[-1]
          
          # get values of parameters for distribution
          params.values = sapply(params.names, function(param) {
            model$getParam(node = node, param = param)
          })
          
          # ensure named vector matches the extracted parameters
          names(params.values) = params.names
          
          # base string for evaluating distribution
          base_dist = getDistributionInfo(distn)
          
          # density function
          dfn = eval(
            expr = parse( 
              text = paste(
                'function(x) ', base_dist$densityName, '(x, ',
                paste(
                  sapply(base_dist$reqdArgs, function(param) {
                    paste("'", param, "' = ", param, '', sep = '')
                  }),
                  collapse = ', '
                ),
                ')', sep = ''
              )
            ), 
            envir = as.list(params.values)
          )
          
          if(base_dist$pqAvail) {
            # quantile function
            qfn = eval(
              expr = parse( 
                text = paste(
                  'function(x) q', 
                  substr(base_dist$densityName, 2, 
                         nchar(base_dist$densityName)), 
                  '(x, ',
                  paste(
                    sapply(base_dist$reqdArgs, function(param) {
                      paste("'", param, "' = ", param, '', sep = '')
                    }),
                    collapse = ', '
                  ),
                  ')', sep = ''
                )
              ), 
              envir = as.list(params.values)
            )
              
            # bounds for 95% equal-tailed interval around prior
            qBounds = qfn(c(.01, .99))
          }
          
        }
        
        if('prior' %in% output) {
          # plot prior distribution
          res$prior = ggplot(data.frame(x = qBounds), aes(x = x)) + 
            stat_function(fun = dfn) + 
            theme_few() + 
            theme(panel.border = element_blank()) + 
            xlab(node) + 
            ylab('Density')
        }
        
        if('prior_post' %in% output) {
          
          # extract posterior samples
          post = samples[-burn, node]
          
          # compute posterior summaries
          hpd = HPDinterval(mcmc(post))
          ess = round(effectiveSize(mcmc(post)))
          
          # posterior density estimate, and indices within HPD interval 
          post.density = density(post)
          dens.inds = which(post.density$x >= hpd[1] & post.density$x <= hpd[2])
          
          # plot prior vs posterior comparison
          res$prior_post = ggplot(data.frame(x = post), aes(x = x)) + 
            geom_ribbon(mapping = aes(x = x, ymin = 0, ymax = d),
                        data = data.frame(x = post.density$x, 
                                          d = post.density$y)[dens.inds,],
                        inherit.aes = FALSE, alpha = .125) +
            stat_density(geom = 'line') + 
            stat_function(fun = dfn, lty = 2) + 
            theme_few() + 
            theme(panel.border = element_blank()) + 
            xlab(node) + 
            ylab('Density') + 
            ggtitle(label = 'Prior (dotted) vs. Posterior (solid)', 
                    subtitle = paste('with 95% HPD (shaded); ESS:', 
                                     prettyNum(ess, big.mark = ',')))
        }
        
        # extract posterior samples
        post = samples[-burn, node]
        
        if('trace' %in% output) {
          res$trace = ggplot(data.frame(x = (1:nrow(samples))[-burn], 
                                        y = post), 
                             aes(x = x, y = y)) +
            geom_line() + 
            xlab('Sample') + 
            ylab(node) + 
            theme_few() + 
            theme(panel.border = element_blank())
        } 
        
        if('post' %in% output) {
          
          # compute posterior summaries
          hpd = HPDinterval(mcmc(post))
          ess = round(effectiveSize(mcmc(post)))
          
          # posterior density estimate, and indices within HPD interval 
          post.density = density(post)
          dens.inds = which(post.density$x >= hpd[1] & post.density$x <= hpd[2])
          
          # plot prior vs posterior comparison
          res$post = ggplot(data.frame(x = post), aes(x = x)) + 
            geom_ribbon(mapping = aes(x = x, ymin = 0, ymax = d),
                        data = data.frame(x = post.density$x, 
                                          d = post.density$y)[dens.inds,],
                        inherit.aes = FALSE, alpha = .125) +
            stat_density(geom = 'line') + 
            theme_few() + 
            theme(panel.border = element_blank()) + 
            xlab(node) + 
            ylab('Density') + 
            ggtitle(label = 'Posterior', 
                    subtitle = paste('with 95% HPD (shaded); ESS:', 
                                     prettyNum(ess, big.mark = ',')))
        }
      } else {
        stop('Input model is NA; cannot extract prior distribution')
      }
    }
    
    res
  })
  
  names(res) = tgt
  res
}
```

  
  
### Posterior densities

```{r bias, eval = FALSE}
library(ggpubr)

pl = distn_info(pattern = 'bias\\[[123]\\]', model = droneLengths, 
                samples = samples, burn = burn, output = 'prior_post')

if(nim_pkg$consts$useBarometer) {
  pl[[1]]$prior_post + xlab(expression(bias[Barometer])) 
}
if(nim_pkg$consts$useLaser) {
  pl[[2]]$prior_post + xlab(expression(bias[Laser]))
}
pl[[3]]$prior_post + xlab(expression(bias[Pixels]))
```

```{r sigma}
library(ggpubr)

pl = distn_info(pattern = '^sigma\\[[123]\\]', model = droneLengths, 
                samples = samples, burn = burn, output = 'prior_post')

if(nim_pkg$consts$useBarometer) {
  pl[[1]]$prior_post + xlab(expression(sigma[Barometer]))
}
if(nim_pkg$consts$useLaser) {
  pl[[2]]$prior_post + xlab(expression(sigma[Laser]))
}
pl[[3]]$prior_post + xlab(expression(sigma[Pixels]))

```

### Transformed and latent posteriors

```{r L}

print('ESS for latent altitudes')
tgt = grep(pattern = '^a', x = samples.names, value = TRUE)
m = mcmc(samples[-burn, tgt, drop = FALSE])
summary(effectiveSize(m))

print('ESS for estimated lengths')
tgt = grep(pattern = paste('L\\[(', 
                           paste(nim_pkg$consts$L_unknown_inds, collapse = '|'),
                           ')\\]', sep = ''), 
           x = samples.names, value = TRUE)
m = mcmc(samples[-burn, tgt, drop = FALSE])
summary(effectiveSize(m))

```

### Posterior length estimates {.tabset}


```{r estimated_length_comparisons}
df = data.frame(
  # measurement id
  pixelId = 1:nrow(nim_pkg$consts$empirical_lengths),
  # empirical estimates of each object's length in image
  empirical = nim_pkg$consts$empirical_lengths[,'Empirical'],
  barometer = nim_pkg$consts$empirical_lengths[,'Barometer'],
  laser = nim_pkg$consts$empirical_lengths[,'Laser'],
  # model id's associated with the measured objects
  objectID = nim_pkg$consts$pixel_id_map[, 'ObjectId']
) %>% 
  # only keep id's/lengths for objects that were estimated in model
  dplyr::filter(objectID %in% nim_pkg$consts$L_unknown_inds) %>% 
  # add posterior means for estimated lengths
  dplyr::mutate(
    modeled = colMeans(samples[-burn, 
      # modeled nodes associated with unknown lengths
      nim_pkg$maps$L[objectID, 'NodeName']
    ])
  ) %>% 
  # munge for plotting
  pivot_longer(cols = empirical:laser, names_to = 'empiricalType', 
               values_to = 'empirical')
  
ggplot(df, aes(y = empirical, x = modeled, col = empiricalType)) + 
  geom_abline(slope = 1, intercept = 0, lty = 3) + 
  geom_point() + 
  ylab('Empirical est. (m)') + 
  xlab('Modeled est. (m)') + 
  theme_few() + 
  theme(panel.border = element_blank()) + 
  facet_wrap(~empiricalType)
```

```{r length_posteriors, results = 'asis', echo = FALSE, eval = FALSE}
for(tg in tgt) {

  cat('####', tg, ' \n\n')

  tg.info = nim_pkg$maps$L %>% dplyr::filter(NodeName == tg)
  cat('Subject: ', tg.info$Subject, ' \n\n')
  cat('Measurement: ', tg.info$Measurement, ' \n\n')

  tg_pattern = tg
  tg_pattern = gsub(pattern = '\\[', replacement = '\\\\[', x = tg_pattern)
  tg_pattern = gsub(pattern = '\\]', replacement = '\\\\]', x = tg_pattern)

  pl = distn_info(pattern = tg_pattern, model = droneLengths,
                  samples = samples, burn = burn, output = 'post')

  print(pl[[1]]$post)

  cat(' \n\n')
}

```



### Length Prediction Tables
```{r, eval = F}

loadd(Mns)  # load expected whale measurements 

# read rds file of predicted lengths
mn_ls <- readRDS(file.path('..', readd(length_samples_mns)))  
IDlist <- names(mn_ls)  # extract AIDs
fulldf <- mn_ls         # save mn_ls to be used in HPD function below

# make a list of names for columns of summary stats table
summcols.names <- c("TL", "TL.05.00..Width", "TL.10.00..Width", "TL.15.00..Width", "TL.20.00..Width", "TL.25.00..Width", "TL.30.00..Width", "TL.35.00..Width", "TL.40.00..Width", "TL.45.00..Width", "TL.50.00..Width", "TL.55.00..Width", "TL.60.00..Width", "TL.65.00..Width", "TL.70.00..Width", "TL.75.00..Width", "TL.80.00..Width", "TL.85.00..Width", "TL.90.00..Width", "TL.95.00..Width")

# make an empty dataframe to fill with the summary stats
summstats.df <- data.frame()

#make a function that calculates the HPD interval and mean of each summary stats column, then adds those values to dataframe
HPDfunc <- function(subdf,tempdf,scol,cname){
  ess = round(effectiveSize(mcmc((subdf[[scol]]))))  # calculate effective sample size (ess) for each column
  tempdf[[paste(cname,".ESS",sep="")]] <- ess    # add column for ess that length column
  hpd = HPDinterval(mcmc(subdf[[scol]])) #calculate HPD interval of column
  tempdf[[paste(cname,".mean",sep="")]] <- mean(subdf[[scol]]) #calculate mean and add column with mean to each length column
  tempdf[[paste(cname,".lower",sep="")]] <- hpd[1] #add column for HPD lower bound value to each length column
  tempdf[[paste(cname,".upper",sep="")]] <- hpd[2] #add columns for HPD upper bound value to each length column
  return(tempdf) #return dataframe with those added columns
}

#loops through list of IDs (loop through each individual's data frame)
for (x in 1:length(IDlist)){
  xx <- IDlist[x] #pull ID name from list
  AID <- xx #save ID as variable
  tempdf <- data.frame(AID) #make empty dataframe with one column containing animal ID, the HPD func will add to this df
  subdf <- fulldf[[xx]] #pull that dataframe of the individual from the named list of dataframes 
  #now we'll loop through the names of the columns we need summary stats for
  for (y in 1:length(summcols.names)){
    scol <- summcols.names[y] #pull the name of the length column
    cname <- summcols.names[y] #pull the name of the column formatted to become the name of the summary stats header
    tempdf <- HPDfunc(subdf,tempdf,scol,cname) #run the HPD function on that column
  }
  summstats.df <- rbind(summstats.df,tempdf) #add the row of summary stats for this individual to the main dataframe
}

```  
```{r eval = F}

# Before linking 'L[#]' to AID/measurements, need to rearrange expected whale measurements dataframe (Mns)
mns_piv <- Mns %>% pivot_longer(cols = starts_with("TL"),
                 names_to = "Measurement", values_to = "Exp_Length") %>%
  mutate(BaroAlt_LHt = round(BaroAlt + Launch_Ht, 2),
         Exp_Length = round(Exp_Length,2)) %>%
  select("AID", "Image", "BaroAlt_LHt", "LaserAlt", "Measurement", "Exp_Length")

# Link "L[#]" to AID + Meas 
mns_rdy <-  nim_pkg$maps$L %>% select(!"Estimated") %>% rename(AID = Subject) %>% 
  left_join(mns_piv, by = c("AID", "Measurement"))

# Now link predicted measurements w/ expected measurements based on AID and measurement
expL <- summstats.df %>% pivot_longer(col = contains("mean"), names_to = "prediction", values_to = "pred_Length") %>% 
  mutate(
    Measurement = sub(".mean", "", prediction),
    pred_Length = round(pred_Length, 2),
  )  %>% select("AID", "Measurement", "pred_Length") %>% left_join(mns_rdy, by = c("AID", "Measurement"))


# Now create similar variables for  HPD lower, HPD upper, & ESS
hpd.low <- summstats.df %>% pivot_longer(col = contains("lower"), names_to = c("prediction"), values_to = "HPD_lower") %>% 
  mutate(
    Measurement = sub(".lower", "", prediction),
    HPD_lower = round(HPD_lower, 2)
  )  %>% select("AID", "Measurement", "HPD_lower")

hpd.up <- summstats.df %>% pivot_longer(col = contains("upper"), names_to = c("prediction"), values_to = "HPD_upper") %>% 
  mutate(
    Measurement = sub(".upper", "", prediction),
    HPD_upper = round(HPD_upper, 2)
  )  %>% select("AID", "Measurement", "HPD_upper")

ess_x <- summstats.df %>% pivot_longer(col = contains("ESS"), names_to = c("prediction"), values_to = "ESS") %>% 
  mutate(
    Measurement = sub(".ESS", "", prediction),
  )  %>% select("AID", "Measurement", "ESS")


# Now merge HPD lower, HPD upper, & ESS and then merge w/ L[#] and TLs
others <- hpd.low %>% left_join(hpd.up, by = c("AID", "Measurement")) %>% 
  mutate(
    HPD_width = HPD_upper - HPD_lower
    ) %>%
  left_join(ess_x, by = c("AID", "Measurement"))

# Merge w/ the predicted lengths, HPDs, ESS w/ expected lengths
tbl <- expL %>% left_join(others, by =c ("AID", "Measurement")) %>% mutate(
  length_diff = round(Exp_Length - pred_Length, 2)
  ) 

# Summary table for report
tbl_report <- tbl %>%
  select(
  "AID", "Image", "Measurement", "NodeName", "BaroAlt_LHt", "LaserAlt", "Exp_Length", "pred_Length", "length_diff", "HPD_width", "ESS")


# summary of predicted and expected lengths
print("Summary table: Expected and Predicted lengths")
knitr::kable(summary(tbl_report[,5:11]), format="markdown")

# table of predicted and expected lengths
print("Expected and Predicted lengths")
knitr::kable(tbl_report, format="markdown")

```

### Plots
```{r, eval = F}

ggplot(data = tbl, aes(x = length_diff)) + geom_histogram(binwidth = 0.05, position = 'identity') + xlab("difference between expected and predicted length")

# Widths
print("Predicted Widths (m)")
tbl %>% dplyr::filter(Measurement != "TL" & Measurement != "TL.95.00..Width") %>% 
  ggplot(aes(x  = Measurement, y = pred_Length, ymin = HPD_lower, ymax = HPD_upper, color = Measurement)) + 
  geom_pointrange() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + scale_x_discrete(labels = c("5","10", "15", "20", "25", "30", "35", "40", "45", "50", "55","60","65", "70", "75", "80", "85", "90")) + ylab("predicted widths (m)") + xlab("% widths") + 
  facet_wrap(~AID)

# TL and HPD widths
print("95% Credible Intervals")
tbl %>% dplyr::filter(Measurement == "TL") %>%  ggplot(aes(x = pred_Length, y = HPD_width)) + # +, color = NodeName)) + 
  geom_point() + ylab("95% Credible Interval width (m)") + xlab("TL (m)") + theme(legend.position="bottom")

```


##### Altitudes
```{r, eval = F}

# Altitude
ggplot(data = tbl, aes(x = LaserAlt, y = BaroAlt_LHt)) + geom_point() + geom_smooth(method = "lm", alpha = 0.50, lty = 2, color = "lightblue") + ggtitle("Expected Altitudes") +
  xlab("Laser altitude (m)") + ylab("Barometer altitude (m)") 


## predicted altitudes w/ expected altitudes
aa = grep(pattern = '^a', x = samples.names, value = TRUE)
maa = mcmc(samples[-burn, aa, drop = FALSE])
pred_a <- as.data.frame(HPDinterval(maa))
pred_a$mean <- colMeans(maa)
pred_a$exp_baro <- nim_pkg$data$a_baro
pred_a$exp_laser <- nim_pkg$data$a_laser
pred_a$hpd_width <- round(pred_a$upper - pred_a$lower, 2)


# expected barometer vs. predicted altitude
ggplot(data = pred_a, aes(x = exp_baro, y = mean, ymin = lower, ymax = upper)) + geom_pointrange() + 
  geom_smooth(method = "lm", alpha = 0.50, lty = 2, color = "lightblue") +
  xlab("expected barometer altitude (m)") + 
  ylab("predicted altitude (m)") + ggtitle("Expected vs. Predicted: barometer") 

# expected laser vs. predicted altitude
ggplot(data = pred_a, aes(x = exp_laser, y = mean, ymin = lower, ymax = upper)) + geom_pointrange() + 
  geom_smooth(method = "lm", alpha = 0.50, lty = 2, color = "lightblue") + 
  xlab("expected laser altitude (m)") + 
  ylab("predicted altitude (m)") + ggtitle("Expected vs. Predicted: laser") 




# writing a temporary csv only to import back in to make "a[#]" an actualy column I can link
write.csv(pred_a, "pred_alts-temp.csv")
alts <- read.csv("pred_alts-temp.csv")

loadd(Calibration_images)
loadd(Mns_images)

alts_key <- alts %>% mutate(
      a = X,
      pred_altitude = mean,
      HPD_lower = lower, 
      HPD_upper = upper,
      HPD_width = hpd_width
  ) %>%
  left_join(rbind(Calibration_images, Mns_images) %>% 
                                 mutate(
                                   exp_laser = AltitudeLaser), 
                               by = "exp_laser"
                               ) %>% 
  select(c("a", "Image", "pred_altitude", "HPD_lower", "HPD_upper", "HPD_width", "exp_baro", "exp_laser"))
```

##### Training vs. testing altitudes 
```{r, eval = F}
# Altitudes and HPD widths
print("95% Credible Intervals")
ggplot(data = alts, aes(x = mean, y = hpd_width, color = X)) + 
  geom_point() + ylab("95% Credible Interval width (m)") + xlab("altitude (m)") + theme(legend.position="bottom")

# add a distinction for test vs. train images
alts_key$img_ty <- "test_img"
alts_key[1:49,]$img_ty <- "train_img"


# HPD widths
ggplot(data = alts_key, aes(x = pred_altitude, y = HPD_width, color = img_ty)) + 
  geom_point() + ylab("95% Credible Interval width (m)") + xlab("altitude (m)") + ggtitle("HPD widths: altitude") + 
  theme(legend.position="bottom")

# expected barometer vs. predicted altitude
ggplot(data = alts_key, aes(x = exp_baro, y = pred_altitude, ymin = HPD_lower, ymax = HPD_upper, color = img_ty)) + geom_pointrange() + 
  geom_smooth(method = "lm", alpha = 0.50, lty = 2, color = "lightblue") + 
  xlab("expected barometer altitude (m)") + 
  ylab("predicted altitude (m)") + ggtitle("Expected vs. Predicted: laser") 

# expected laser vs. predicted altitude
ggplot(data = alts_key, aes(x = exp_laser, y = pred_altitude, ymin = HPD_lower, ymax = HPD_upper, color = img_ty)) + geom_pointrange() + 
  geom_smooth(method = "lm", alpha = 0.50, lty = 2, color = "lightblue") + 
  xlab("expected laser altitude (m)") + 
  ylab("predicted altitude (m)") + ggtitle("Expected vs. Predicted: laser") 
```




### Altitude Prediction Tables
```{r, eval = F}

# Summary of altitudes
print("Summary Table: Expected and Predicted altitudes")
knitr::kable(summary(alts_key), format="markdown")  


# table and summary 
print("Expected and Predicted altitudes")
knitr::kable(alts_key, format="markdown")
```


### Traceplots

```{r bias_trace, eval = FALSE}
library(ggpubr)

pl = distn_info(pattern = 'bias\\[[123]\\]', model = droneLengths, 
                samples = samples, burn = burn, output = 'trace')

pl[[1]]$trace + ylab(expression(bias[Barometer]))
pl[[2]]$trace + ylab(expression(bias[Laser])) 
pl[[3]]$trace + ylab(expression(bias[Pixels]))
```

```{r sigma_trace, eval = TRUE}
library(ggpubr)

pl = distn_info(pattern = '^sigma\\[[123]\\]', model = droneLengths, 
                samples = samples, burn = burn, output = 'trace')

pl[[1]]$trace + ylab(expression(sigma[Barometer]))
pl[[2]]$trace + ylab(expression(sigma[Laser])) 
pl[[3]]$trace + ylab(expression(sigma[Pixels]))
```